{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP03 : Régression logistique\n",
    "\n",
    "## Régression linéaire polynomiale \n",
    "Dans cette partie, comme expliqué en cours nous voyons un exemple simple d'un modèle linéaire sous forme d'un polynome multiple. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression  \n",
    "from sklearn.preprocessing import PolynomialFeatures \n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [i for i in range(10)]\n",
    "Y = [random.gauss(x,0.75) for x in X]\n",
    "\n",
    "X = np.asarray(X)\n",
    "Y = np.asarray(Y)\n",
    "\n",
    "X = X[:,np.newaxis]\n",
    "Y = Y[:,np.newaxis]\n",
    "\n",
    "plt.scatter(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 4 #degrès du polynome résultant.\n",
    "\n",
    "polynomial_features = PolynomialFeatures(degree = degree)\n",
    "X_TRANSF = polynomial_features.fit_transform(X)\n",
    "model = LinearRegression()\n",
    "model.fit(X_TRANSF, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_NEW = model.predict(X_TRANSF)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(Y,Y_NEW))\n",
    "r2 = r2_score(Y,Y_NEW)\n",
    "\n",
    "print('RMSE: ', rmse)\n",
    "print('R2: ', r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new_min = 0.0\n",
    "x_new_max = 10.0\n",
    "\n",
    "X_NEW = np.linspace(x_new_min, x_new_max, 100)\n",
    "X_NEW = X_NEW[:,np.newaxis]\n",
    "\n",
    "X_NEW_TRANSF = polynomial_features.fit_transform(X_NEW)\n",
    "\n",
    "Y_NEW = model.predict(X_NEW_TRANSF)\n",
    "\n",
    "plt.plot(X_NEW, Y_NEW, color='coral', linewidth=3)\n",
    "\n",
    "plt.grid()\n",
    "plt.xlim(x_new_min,x_new_max)\n",
    "plt.ylim(0,10)\n",
    "\n",
    "title = 'Degree = {}; RMSE = {}; R2 = {}'.format(degree, round(rmse,2), round(r2,2))\n",
    "\n",
    "plt.title(\"Polynomial Linear Regression using scikit-learn and python 3 \\n \" + title,\n",
    "          fontsize=10)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.scatter(X,Y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Régression logistique \n",
    "\n",
    "La régression logistique est généralement utilisée à des fins de classification. Contrairement à la régression linéaire, la variable à prédire ne peut prendre qu'un nombre limité de valeurs (valeurs discrètes). \n",
    "\n",
    "Lorsque le nombre de résultats possibles est seulement deux, on parle de régression logistique binaire.\n",
    "\n",
    "![](img/logistic.JPG) \n",
    "\n",
    "Dans la figure ci-dessus on comprend que la régression logistique est composée d'une régression linéaire suivie de l'application d'une certaine fonction. Cette fonction est la fonction sigmoid dont voici le graphe : \n",
    "\n",
    "![](img/sigmoid.JPG) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Préparation des données : \n",
    "Les données consistent en un ensemble de notes des etudiants et la valeur à prédire est si l'etudiant est admis(1) ou pas(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = [\"Note1\", \"Note2\", \"Admis\"]\n",
    "notes = pd.read_csv('datasets/marks.txt', names=header)\n",
    "notes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = notes.iloc[:, :-1]\n",
    "y = notes.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "admis = notes.loc[y == 1]\n",
    "non_admis = notes.loc[y == 0]\n",
    "\n",
    "plt.scatter(admis.iloc[:, 0], admis.iloc[:, 1], s=10, label='Admis')\n",
    "plt.scatter(non_admis.iloc[:, 0], non_admis.iloc[:, 1], s=10, label='Non Admis')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- Régression logistique \n",
    "\n",
    "**Expressions mathématiques :  **:\n",
    "<img src=\"img/Math.PNG\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "<img src=\"img/cost.PNG\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "<img src=\"img/total.PNG\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "<img src=\"img/deriv.PNG\" alt=\"Drawing\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : Calculer le sigmoid de la valeur x \n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Test : \n",
    "sigmoid(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : La fonction de coût utilisée dans la régression logistique \n",
    "def J(x,y,theta):\n",
    "    z = np.dot(x,theta)\n",
    "    h = sigmoid(z)\n",
    "    return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.zeros((X.shape[1]+1,1))\n",
    "#RESHAPE Y\n",
    "y = y[:,np.newaxis]\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(theta.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import fmin_tnc\n",
    "# TODO : Programmer la fonction d'entrainement du modèle \n",
    "def train(x,y, theta):\n",
    "    lr = 0.01\n",
    "    for i in range(10000):\n",
    "        z = np.dot(x,theta)\n",
    "        h = sigmoid(z)\n",
    "        gradient = np.dot(x.T,(h-y)) / y.size\n",
    "        theta -= lr * gradient\n",
    "        \n",
    "    return theta\n",
    "\n",
    "# TODO : fonction de normalisation des données X \n",
    "def normalisation(X):\n",
    "    mins = np.min(X, axis = 0)\n",
    "    maxs = np.max(X, axis = 0)\n",
    "    rng = maxs - mins\n",
    "    norm_X = 1 - ((maxs - X)/rng)\n",
    "    return norm_X \n",
    "\n",
    "# TODO : Entrainer le modèle en choisissant les bons hyperparamètres. \n",
    "norm_X = normalisation(X)\n",
    "#AJOUTER des 1 pour le parametre theta-zero\n",
    "norm_X= np.append(np.ones((norm_X.shape[0],1)),norm_X,axis=1)\n",
    "theta = train(norm_X,y,theta)\n",
    "\n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = np.array([np.min(X.values[:,1]),np.max(X.values[:,1])])\n",
    "y_values = (- (theta[0] + np.dot(theta[1], x_values[0])) / theta[2],  - (theta[0] + np.dot(theta[1], x_values[1])) / theta[2])\n",
    "\n",
    "plt.plot(x_values, y_values, label='ligne de décision')\n",
    "plt.xlabel('Note 1 ')\n",
    "plt.ylabel('Note 2 ')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : donner la fonction de prédiction qui retourne la probabilité que X est dans chaque classe \n",
    "def predict(x):\n",
    "    z = np.dot(x,theta)\n",
    "    h = sigmoid(z)\n",
    "    if h>0.5:\n",
    "        return 1\n",
    "    else :\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- Implementation sous sklearn : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# TODO : Diviser les données en données d'entrainement et données de tests (Fait dans le TP02 )\n",
    "## Décider de la taille des données pour chaque set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# Créer le modèle \n",
    "model = LogisticRegression()\n",
    "\n",
    "# Entraîner le modèle \n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Prédire les classes \n",
    "predicted_classes = model.predict(X_test)\n",
    "\n",
    "# Calculer le score du modèle \n",
    "accuracy = accuracy_score(y_test,predicted_classes)\n",
    "\n",
    "print('le score du modèle : ',accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
